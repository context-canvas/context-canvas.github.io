<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/kavanavenkatesh/">Kavana Venkatesh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yusufdalva.github.io/">Yusuf Dalva</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://isminoula.github.io/">Ismini Lourentzou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://pinguar.org/">Pinar Yanardag</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Virginia Tech,</span>
            <span class="author-block"><sup>2</sup>University of Illinois Urbana-Champaign</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.09614"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.09614"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <!-- TL;DR Section -->
      <h2 class="title is-5" style="font-weight: 400; text-align: justify; margin-bottom: 20px; color: #444;">
        <strong>TL;DR</strong> We propose a training-free approach that leverages knowledge graph-based Retrieval Augmented Generation to 
        enhance image generation and editing of text-to-image diffusion models. We also introduce a novel RAG context-guided self-correction mechanism. 
        The approach enables generation of contextually and narratively accurate images for
        complex, domain-specific scenarios that standard T2I models struggle with, using simple high-level user prompts.
      </h2>
      <!-- Teaser image -->
      <img id="teaser-image" src="./static/images/paper-teaser.jpg" alt="Teaser Image" style="width: 120%; height: auto;">
      <h2 class="subtitle teaser-caption" style="font-size: 1rem; text-align: justify;">
      <span class="dnerf">Context Canvas</span> significantly improves image generation for domain-specific, complex characters that T2I models might
        otherwise struggle with (top left). Additionally, our method enables disentangled image editing by extracting precise item descriptions,
        such as transforming a simple “add a sword” prompt into a fire sword for the character Jambavan. It also adeptly captures relationships
        by utilizing the graph, for instance, generating Jambavan’s daughter from a simple “with his daughter” prompt, without needing explicit
        details about her appearance (bottom middle). Moreover, we introduce a novel RAG-based self-correction technique that further refines
        images to improve visual and narrative accuracy (right).
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a novel approach to enhance the capabilities of text-to-image models by incorporating Retrieval-Augmented Generation 
            with a knowledge graph. Our system dynamically retrieves detailed character information
            and relational data from the knowledge graph, enabling the
            generation of visually accurate and contextually rich images. Furthermore, we propose a self-correcting mechanism 
            within Stable Diffusion models to ensure consistency
            and fidelity in visual outputs, leveraging the rich context
            from the graph to guide corrections. To our knowledge, <span class="dnerf">Context Canvas</span> represents 
            the first application of graph-based RAG in enhancing
            T2I models, representing a significant advancement for producing high-fidelity, context-aware multi-faceted images.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column">
          <!-- Paper Method Diagram -->
          <h2 class="title is-3 has-text-centered">Method</h2>
          <div class="publication-diagram" style="text-align: center;">
            <img id="method-diagram" src="./static/images/overview-diagram.jpg" alt="Paper Method Diagram" style="width: 100%; height: auto; margin-top: 20px;">
            <p class="teaser-caption" style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px; color: #444;">
              Context Canvas introduces a novel knowledge graph-based RAG framework to 
              enhance T2I diffusion models for context-driven generation and editing of images with complex, 
              domain-specific concepts. The framework includes three key stages: 
              knowledge graph-driven image generation, context-aware image editing, and a novel self-correction mechanism. 
              In the first stage, user prompts are enriched with detailed character attributes and relationships from the knowledge graph, 
              enabling precise depictions. The editing stage seamlessly integrates specific items or features into images by retrieving relevant contextual data. 
              The self-correction process iteratively refines images using RAG context-guided prompts to ensure alignment 
              with intended narratives and visual fidelity, addressing complex character traits and ensuring coherence across 
              cultural and contextual dimensions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <!-- Qualitative Results Main Heading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Qualitative Results</h2>
      </div>
    </div>

    <!-- Image Generation Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">Image Generation</h3>
      </div>
    </div>

    <!-- Image Generation Results -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img id="image-generation" src="./static/images/img-generation.jpg" alt="Image Generation" style="width: 120%; height: auto;">
          <p class="teaser-caption">
          <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px; color: #444;">
            Context Canvas enhances image generation across diverse domains by integrating cultural and contextual details often missed by
            standard models. For example, it accurately portrays rare Indian mythological characters like Tumburu with his horse-face and instrument,
            and Gandabherunda as a dual-headed bird (top left). Domains such as mythology often comprise characters with multiple forms 
            (eg.,celestial and human form). Our method picks up subtle cues from user prompts to depict such characters in the right form. For instance, 
            Indian mythology character, 'Ganga' has a heavenly river form and a human form (Duality), both of which it represents accuratelyIt (left middle). 
            Our method adapts to various mythologies, capturing Melinoe’s ghostly essence (left bottom) and Zhong Kui’s
            fierce warrior form (middle bottom). In Project Gutenberg domains, such as Historical Fiction, Gothic Horror, and Fantasy, it captures
            narrative-specific details like Captain Ahab’s ivory leg and gaunt expression (top 4th column) and Edmond Dant`es’ pale skin, coat, and ring
            (top 3rd column). For Gothic Horror, it enhances Count Dracula’s menacing presence (middle 3rd column) and infers Manfred’s guilty,
            dark persona (middle 4th column). Our approach faithfully represents Lilith with bat wings and snakes
            and Lizarel with ethereal beauty and silvery hair (bottom right), demonstrating superior fidelity across cultural and literary domains.
          </p>
        </div>
      </div>
    </div>


    <!-- Self-Correction Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">Self-Correcting RAG-Guided Diffusion (SRD)</h3>
      </div>
    </div>
    
    <!-- Self-Correction Results -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img id="self-correction" src="./static/images/self-correction.jpg" alt="Self-Correction Results" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption">
          <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px; color: #444;">
            SRD achieves highly accurate and culturally resonant depictions of complex mythological characters
            through iterative, context-rich prompt refinement. SRD corrects Vritra’s multi-headed form, drawing on the story’s cultural narrative 
            from a robust knowledge graph to identify and position Indra accurately. For ‘Garuda’, iterative
            adjustments restore his iconic gold jewelry and vibrant wing color, enhancing his divine representation. For
            “Yama on his vehicle”, our approach automatically identifies his vehicle and adds essential symbols like his crown, mace, and noose, with precise
            skin tone adjustments. Similarly for ‘Mahishasura’, his incorrect tail is transformed into a snake he is typically depicted with.
          </p>
        </div>
      </div>
    </div>


    <!-- Image Editing -->
    <div class="columns is-centered" id="image-editing" style="margin-top: 30px;">
    <div class="container is-max-widescreen">
      <div class="column">
        <!-- Editing -->
        <h3 class="title is-4 has-text-centered">Editing</h3>
        <img id="editing-results" src="./static/images/image-editing.jpg" alt="Editing Results" style="width: 100%; height: auto; margin-top: 20px;">
        <h2 class="subtitle result-caption" style="font-size: 1rem; text-align: justify; margin-top: 10px; max-width: 1200px; margin-left: auto; margin-right: auto;">
          Our method enhances ControlNet’s disentangled editing by adding culturally accurate elements. In the first row, for
          Jambavan, standard ControlNet introduces generic items. In the second row (bottom right), our approach accurately depicts
          ‘his sword’ as a fire sword, ‘primary weapon’ as a mace and ‘his daughter’ as an adopted human. For ‘Shiva’ (bottom row), the system
          intuitively adds his son ‘Ganesha’, snake ‘Vasuki’ and the ‘damaru’ in his mountain ‘abode’ without explicit instructions, while standard
          ControlNet either adds generic objects or fails to make any edits at all.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <!-- Qualitative Comparison Main Heading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Qualitative Comparison</h2>
      </div>
    </div>

    <!-- Comparison Image -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content" style="text-align: justify;">
          <img id="qualitative-comparison" src="./static/images/qualitative-comparison.jpg" alt="Qualitative Comparison" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption">
            This figure highlights the comparative performance of <span class="dnerf">Context Canvas</span> against state-of-the-art methods. 
            Our approach consistently achieves superior results in capturing contextually accurate details, including intricate features like character-specific appearances and attributes. For example, when depicting Tumburu or Mahishasura, Context Canvas preserves their unique traits with higher fidelity compared to Flux and ControlNet.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <!-- Quantitative Results Main Heading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Quantitative Results</h2>
      </div>
    </div>

  <!-- Explanation for Qualitative Results -->
  <div class="columns is-centered">
    <div class="column">
      <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 20px;">
      We conduct a comprehensive evaluation of 'Context Canvas' in two stages, assessing both its foundational RAG component and 
      image generation. We benchmark our approach against SOTA T2I models-such as Flux, SDXL, and DALL-E 3 across image generation 
      and two rounds of self-correction. 
      </p>
    </div>
  </div>

   <!-- RAG Evaluation Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">RAG Evaluation</h3>
      </div>
    </div>

   <!-- Explanation for RAG Evaluation -->
    <div class="columns is-centered">
      <div class="column">
        <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 20px;">
          We evaluate the foundational RAG system used in our method for both 
          retrieval and generation using <a href="https://arxiv.org/abs/2303.16634" target="_blank" style="color: #007bff; text-decoration: underline;">GEval</a>.
          Our RAG process achieves high evaluation scores across both retrieval and generation due to meticulous data curation, retrieval, and prompt engineering. 
        </p>
      </div>
    </div>

    
    <!-- RAG Evaluation Table with Scaled Caption -->
    <div class="columns is-centered">
      <div class="column is-narrow">
        <div class="content" style="text-align: center; width: 30%; margin: 0 auto;">
          <img id="rag-evaluation" src="./static/images/rag-quant-table.jpg" alt="RAG Evaluation Table" style="width: 100%; height: auto; margin-top: 20px;">
        </div>
      </div>
    </div>


    <!-- Benchmarking Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">Comparison with SOTA T2I Models</h3>
      </div>
    </div>

    <!-- Explanation for SOTA Benchmarking -->
    <div class="columns is-centered">
      <div class="column">
        <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 20px;">
          Traditional metrics like CLIP Score and FID, while effective for general image-text alignment and quality, fail to capture the cultural specificity, 
          narrative depth, and relational accuracy crucial for domain-specific tasks. To address these limitations, we utilize the 
          <a href="https://arxiv.org/abs/2306.05685" target="_blank" style="color: #007bff; text-decoration: underline;">LLM-as-a-Judge</a> 
          framework, by defining metrics such as Attribute Accuracy, Context Relevance, Visual Fidelity, and Intent Representation. We implement the metrics 
          using <a href="https://github.com/confident-ai/deepeval" target="_blank" style="color: #007bff; text-decoration: underline;">DeepEval</a> library. 
          These metrics assess nuanced attributes like character-specific elements, situational alignment, and the overall essence of the generated 
          imagery.
        </p>
      </div>
    </div>

    <!-- SOTA Comparison Table -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img id="sota-benchmarking" src="./static/images/image-gen-quant-table.jpg" alt="SOTA Benchmarking Table" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption" style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px;">
           We compare <span class="dnerf">Context Canvas</span> with SOTA T2I models across key quantitative metrics. 
            Our framework achieves superior scores in cultural and contextual fidelity, image coherence, and narrative-specific accuracy, 
            demonstrating its robustness and adaptability across diverse domains.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{venkatesh2024contextcanvasenhancingtexttoimage,
              title={Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG}, 
              author={Kavana Venkatesh and Yusuf Dalva and Ismini Lourentzou and Pinar Yanardag},
              year={2024},
              eprint={2412.09614},
              archivePrefix={arXiv},
              primaryClass={cs.CV},
              url={https://arxiv.org/abs/2412.09614} 
        }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website, 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
